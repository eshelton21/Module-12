Module 12
Intro to Linear Regression

Preliminaries: install the {curl}, {ggplot2}, {gridExtra}, {manipulate}, and {lmodel2} packages.
Objectives: discuss the use of simple linear regression to explore the relationship among two continuous variables: a single predictor variable and a single response variable.

Covariance and Correlation
  Regression modeling is one of the most important set of tools for looking at relationships among two or more variables. Simple bivariate plot of height by weight in the zombies dataset:
    > library(curl)
    > library(ggplot2)
    > f <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN597_Fall19/zombies.csv")
    > d<-read.csv(f, header = TRUE, sep = ",", stringsAsFactors = FALSE)
    > head(d)
    > plot(data = d, height ~ weight) [ETA: returns a scatterplot; weight on x, height on y]
  Covariance: expresses how much two numeric variables 'change together' and whether that change is positive or negative.
  Variance: var(x) = sum((x-mean(x))^2/(n-1))
  Covariance: cov(x, y) = sum(((x-mean(x))(y-mean(y)))/(n-1))
  Challenge 1: What is the covariance between zombie weight and zombie height? What does it mean if it's positive or negatice? Does the order of the variables matter?
    > w<-d$weight
    > h<-d$height
    > n<-length(w) [ETA: could also be length(h)]
    > cov_wh<-sum((w-mean(w))*(h-mean(h))/(n-1))
    > cov_wh
    [1] 66.03314
    > cov(w,h) [ETA: built-in R function]
    [1] 66.03314
  Correlation coefficient: standardized form of the covariance, which summarizes on a standard scale, -1 to +1, the strength and direction of a relationship.
  Correlation: covariance divided by the product of the stdev of both variables. cor(x,y) = cov(x,y)/(sd(x)sd(y))
  Challenge 2: calculate the correlation between zombie height and weight.
    > sd_w<-sd(w)
    > sd_h<-sd(h)
    > cor_wh<-cov_wh/(sd_w*sd_h)
    > cor_wh
    [1] 0.8325862
    > cor(w,h) [ETA: built-in R function]
    [1] 0.8325862
    > cor(w, h, method = "pearson") 
    [1] 0.8325862
  The above formulation of the correlation coefficient is Pearson's product-moment correlation coefficient and is often abbreviated as rho (ρ). There are other nonparametric forms of the correlation coefficient we might also calculate.
    > cor(w, h, method = "kendall")
    [1] 0.6331932
    > cor(w, h, method = "spearman")
    [1] 0.82668
    
Regression
  Regression: the set of tools that lets us explore the relationships between variables further; regression analysis typically involves identifying and exploring linear models, or functions, that describe the relationship between two variables. This can be used to use one or more variables to predict the value of another, to develop and choose among different models of the relationship, and to do analyses of covariation among sets of variables to identify their relative explanatory power.
  Linear regression: come up with a model or function that estimates the mean value of one variable, the response/outcome variable, given the particular value of another variable or set of variables, the predictor variable.
  Bivariate regression: single predictor and single response variable.Y=β0+β1Xi+ϵi; Y is the linear function and X is the predictor/independent variable; the first β0 is the y intercept; β1 is the slope of the line; ϵi is a normal random variable ~N(0, sigma^2), with stdev assumed to be constant across all values of X. The process of estimating the beta regression coefficients is called 'fitting the model'.
  Ordinary least squares (OLS) regression: finding the best fit line whose coefficients minimize the sum of the squared deviations of each observation in the Y direction from that predicted by the line. E(y-(b1x + b0))^2; E(height-(b1*weight + b0))^2. We can estimate the slope by subtracting the mean from each value.
    > y<-h-mean(h)
    > x<-w-mean(w)
    > z<-data.frame(cbind(x,y))
    > g<-ggplot(data = z, aes(x = x, y = y)) + geom_point()
    > g [ETA: returns a scatterpoint plot]
  Then minimize: E(y centered - (b1*x centered))^2
    > slope.test<-function(beta1) {
    +     g <- ggplot(data = z, aes(x = x, y = y))
    +     g <- g + geom_point()
    +     g <- g + geom_abline(intercept = 0, slope = beta1, size = 1, colour = "blue", 
    +                          alpha = 1/2)
    +     ols <- sum((y - beta1 * x)^2)
    +     g <- g + ggtitle(paste("Slope = ", beta1, "\nSum of Squared Deviations = ", 
    +                            round(ols, 3)))
    +     g
    + }
    > manipulate(slope.test(beta1), beta1 = slider(-1,1,initial = 0, step = 0.005))
    > beta1<-cor(w,h)*(sd(h)/sd(w))
    > beta1
    [1] 0.1950187
    > beta1<-cov(w,h)/var(w)
    > beta1
    [1] 0.1950187
    > beta1<-sum((h-mean(h))*(w-mean(w)))/sum((w-mean(w))^2)
    > beta1
    [1] 0.1950187
  To find b0: mean(y)-b1*mean(x)
    > beta0<-mean(h)-beta1*mean(w)
    > beta0
    [1] 39.56545
  Model 1 regression: deviation measured perpendicular to one of the axes, used when the levels of the predictor variable are either measured without error, or with less error than the response variable, or are set by the researcher (eg for defined treatment variables in an ecological experiment).

The lm() Function
  lm(): does all of the previous calculations.
    > m<-lm(height ~ weight, data = d)
    > m

    Call:
    lm(formula = height ~ weight, data = d)

    Coefficients:
    (Intercept)       weight  
         39.565        0.195  
    > names(m)
    [1] "coefficients"  "residuals"     "effects"      
    [4] "rank"          "fitted.values" "assign"       
    [7] "qr"            "df.residual"   "xlevels"      
    [10] "call"          "terms"         "model"        
    > m$coefficients
    (Intercept)      weight 
     39.5654460   0.1950187 
    > head(m$model)
        height   weight
    1 62.88951 132.0872
    2 67.80277 146.3753
    3 72.12908 152.9370
    4 66.78484 129.7418
    5 64.71832 132.4265
    6 71.24326 152.5246
    > g<-ggplot(data = d, aes(x = weight, y = height))
    > g<-g+geom_point()
    > g<-g+geom_smooth(method = "lm", formula = y ~ x)
    > g