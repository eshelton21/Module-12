Module 12
Intro to Linear Regression

Preliminaries: install the {curl}, {ggplot2}, {gridExtra}, {manipulate}, and {lmodel2} packages.
Objectives: discuss the use of simple linear regression to explore the relationship among two continuous variables: a single predictor variable and a single response variable.

Covariance and Correlation
  Regression modeling is one of the most important set of tools for looking at relationships among two or more variables. Simple bivariate plot of height by weight in the zombies dataset:
    > library(curl)
    > library(ggplot2)
    > f <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN597_Fall19/zombies.csv")
    > d<-read.csv(f, header = TRUE, sep = ",", stringsAsFactors = FALSE)
    > head(d)
    > plot(data = d, height ~ weight) [ETA: returns a scatterplot; weight on x, height on y]
  Covariance: expresses how much two numeric variables 'change together' and whether that change is positive or negative.
  Variance: var(x) = sum((x-mean(x))^2/(n-1))
  Covariance: cov(x, y) = sum(((x-mean(x))(y-mean(y)))/(n-1))
  Challenge 1: What is the covariance between zombie weight and zombie height? What does it mean if it's positive or negatice? Does the order of the variables matter?
    > w<-d$weight
    > h<-d$height
    > n<-length(w) [ETA: could also be length(h)]
    > cov_wh<-sum((w-mean(w))*(h-mean(h))/(n-1))
    > cov_wh
    [1] 66.03314
    > cov(w,h) [ETA: built-in R function]
    [1] 66.03314
  Correlation coefficient: standardized form of the covariance, which summarizes on a standard scale, -1 to +1, the strength and direction of a relationship.
  Correlation: covariance divided by the product of the stdev of both variables. cor(x,y) = cov(x,y)/(sd(x)sd(y))
  Challenge 2: calculate the correlation between zombie height and weight.
    > sd_w<-sd(w)
    > sd_h<-sd(h)
    > cor_wh<-cov_wh/(sd_w*sd_h)
    > cor_wh
    [1] 0.8325862
    > cor(w,h) [ETA: built-in R function]
    [1] 0.8325862
    > cor(w, h, method = "pearson") 
    [1] 0.8325862
  The above formulation of the correlation coefficient is Pearson's product-moment correlation coefficient and is often abbreviated as rho (ρ). There are other nonparametric forms of the correlation coefficient we might also calculate.
    > cor(w, h, method = "kendall")
    [1] 0.6331932
    > cor(w, h, method = "spearman")
    [1] 0.82668
    
Regression
  Regression: the set of tools that lets us explore the relationships between variables further; regression analysis typically involves identifying and exploring linear models, or functions, that describe the relationship between two variables. This can be used to use one or more variables to predict the value of another, to develop and choose among different models of the relationship, and to do analyses of covariation among sets of variables to identify their relative explanatory power.
  Linear regression: come up with a model or function that estimates the mean value of one variable, the response/outcome variable, given the particular value of another variable or set of variables, the predictor variable.
  Bivariate regression: single predictor and single response variable.Y=β0+β1Xi+ϵi; Y is the linear function and X is the predictor/independent variable; the first β0 is the y intercept; β1 is the slope of the line; ϵi is a normal random variable ~N(0, sigma^2), with stdev assumed to be constant across all values of X. The process of estimating the beta regression coefficients is called 'fitting the model'.
  Ordinary least squares (OLS) regression: finding the best fit line whose coefficients minimize the sum of the squared deviations of each observation in the Y direction from that predicted by the line. E(y-(b1x + b0))^2; E(height-(b1*weight + b0))^2. We can estimate the slope by subtracting the mean from each value.
    > y<-h-mean(h)
    > x<-w-mean(w)
    > z<-data.frame(cbind(x,y))
    > g<-ggplot(data = z, aes(x = x, y = y)) + geom_point()
    > g [ETA: returns a scatterpoint plot]
  Then minimize: E(y centered - (b1*x centered))^2
    > slope.test<-function(beta1) {
    +     g <- ggplot(data = z, aes(x = x, y = y))
    +     g <- g + geom_point()
    +     g <- g + geom_abline(intercept = 0, slope = beta1, size = 1, colour = "blue", 
    +                          alpha = 1/2)
    +     ols <- sum((y - beta1 * x)^2)
    +     g <- g + ggtitle(paste("Slope = ", beta1, "\nSum of Squared Deviations = ", 
    +                            round(ols, 3)))
    +     g
    + }
    > manipulate(slope.test(beta1), beta1 = slider(-1,1,initial = 0, step = 0.005))
    > beta1<-cor(w,h)*(sd(h)/sd(w))
    > beta1
    [1] 0.1950187
    > beta1<-cov(w,h)/var(w)
    > beta1
    [1] 0.1950187
    > beta1<-sum((h-mean(h))*(w-mean(w)))/sum((w-mean(w))^2)
    > beta1
    [1] 0.1950187
  To find b0: mean(y)-b1*mean(x)
    > beta0<-mean(h)-beta1*mean(w)
    > beta0
    [1] 39.56545
  Model 1 regression: deviation measured perpendicular to one of the axes, used when the levels of the predictor variable are either measured without error, or with less error than the response variable, or are set by the researcher (eg for defined treatment variables in an ecological experiment).

The lm() Function
  lm(): does all of the previous calculations.
    > m<-lm(height ~ weight, data = d)
    > m

    Call:
    lm(formula = height ~ weight, data = d)

    Coefficients:
    (Intercept)       weight  
         39.565        0.195  
    > names(m)
    [1] "coefficients"  "residuals"     "effects"      
    [4] "rank"          "fitted.values" "assign"       
    [7] "qr"            "df.residual"   "xlevels"      
    [10] "call"          "terms"         "model"        
    > m$coefficients
    (Intercept)      weight 
     39.5654460   0.1950187 
    > head(m$model)
        height   weight
    1 62.88951 132.0872
    2 67.80277 146.3753
    3 72.12908 152.9370
    4 66.78484 129.7418
    5 64.71832 132.4265
    6 71.24326 152.5246
    > g<-ggplot(data = d, aes(x = weight, y = height))
    > g<-g+geom_point()
    > g<-g+geom_smooth(method = "lm", formula = y ~ x)
    > g
  Model II regression analysis: used when the predictor variable also needs to be calculated, a line of best fit is chosen that minimizes in some way the direct distance of each point to the best fit line; there are several types of Model II regression, known as major axis, ranged major axis, and reduced/standard major axis. 
  The {lmodel2} package does Model I and II regression analysis; the significance of the regression coefficients is determined based on permutation.
    > library(lmodel2)
    > mII<-lmodel2(height ~ weight, data = d, range.y = "relative", range.x = "relative", nperm = 1000)
    > mII: returns the Model II regression significant values
    > plot(mII, "OLS"): returns a scatterplot with a line of best fit
    > plot(mII, "RMA"): returns a slightly different scatterplot with a line of best fit
    > plot(mII, "SMA"): [see above]
    > plot(mII, "MA"): [see above]
  Note: using OLS yields equivalent results to the Model I regression using lm() above.
    > mI<-lm(height ~ weight, data = d)
    > summary(mI): returns significant values of the data
    > par(mfrow = c(1,2))
    > plot(mII, main = "lmodel2() OLS")
    > plot(data = d, height ~ weight, main = "lm()")
    > abline(mI): returns two regression graphs, both with lines of best fit, although the OLS one is a triple line in red and gray
  Challenge 3: plot zombie height as a function of age, derive by hand the ordinary least squares regression coefficients b1 and b0 for this data, confirm that you get the same results using lm(), and repeat the analysis for males and females separately.
    > par(mfrow = c(1,1))
    > plot(data = d, height ~ age)
    > head(d): returns same as module
    > beta1<-cor(d$height, d$age)*sd(d$height)/sd(d$age)
    > beta1
    [1] 0.9425086
    > beta0<-mean(d$height)-beta1*mean(d$age)
    > beta0
    [1] 48.73566
    > m<-lm(height ~ age, data = d)
    > m

    Call:
    lm(formula = height ~ age, data = d)

    Coefficients:
    (Intercept)          age  
        48.7357       0.9425 

Statistical Inference in Regression
  Regression coefficients can only be used as predictors if there's statistical evidence the variables are related. Then they can also be used for statistical inference; extending estimates from a sample to a population. The lm() function provides info useful for inference.
    > m<-lm(data = d, height ~ weight)
    > summary(m)

    Call:
    lm(formula = height ~ weight, data = d)

    Residuals:
        Min      1Q  Median      3Q     Max 
    -7.1519 -1.5206 -0.0535  1.5167  9.4439 

    Coefficients:
                 Estimate Std. Error t value Pr(>|t|)    
    (Intercept) 39.565446   0.595815   66.41   <2e-16 ***
    weight       0.195019   0.004107   47.49   <2e-16 ***
    ---
    Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

    Residual standard error: 2.389 on 998 degrees of freedom
    Multiple R-squared:  0.6932,	Adjusted R-squared:  0.6929 
    F-statistic:  2255 on 1 and 998 DF,  p-value: < 2.2e-16
  Coefficient of determination: 'R-squared' value, summary of the total variation in the y variable that is explained by the x variable. In the example, ~69% of the variation in zombie height is explainable by weight.
  The summary(m) also gives the standard error of the regression coefficient, with t and p values. These can also be calculated by hand.
    > t<-coef(summary(m))
    > t<-data.frame(unlist(t))
    > colnames(t)<-c("Est", "SE", "t", "p")
    > t
                       Est          SE        t             p
    (Intercept) 39.5654460 0.595814678 66.40562  0.000000e+00
    weight       0.1950187 0.004106858 47.48611 2.646279e-258
    > t$calct<-(t$Est-0)/t$SE
    > t$calcp<-2*pt(t$calct, df = 998, lower.tail = FALSE) [ETA: 2* because it's a 2-tailed test]
    > t
                       Est          SE        t             p    calct
    (Intercept) 39.5654460 0.595814678 66.40562  0.000000e+00 66.40562
    weight       0.1950187 0.004106858 47.48611 2.646279e-258 47.48611
                        calcp
    (Intercept)  0.000000e+00
    weight      2.646279e-258
  The confidence intervals can also be calculated.
    > t$lower<-t$Est - qt(0.975, df = 998)*t$SE
    > t$upper<-t$Est + qt(0.975, df = 998)*t$SE
    > ci<-c(t$lower, t$upper) [ETA: calculates ci by hand]
    > ci
    [1] 38.3962527  0.1869597 40.7346393  0.2030778
    > ci<-confint(m, level = 0.95) [ETA: calculates ci by results of lm()]
    > ci
                     2.5 %     97.5 %
    (Intercept) 38.3962527 40.7346393
    weight       0.1869597  0.2030778
    
Interpreting Regression Coefficients and Prediction
  beta0: the intercept, the predicted value of y when x is zero.
  beta1: the slope, the expected change in units of y for every one unit of change in x.
  The overall equation calculates predicted values of y for new observations of x, CIs around the predicted mean value of y for each value of x, and prediction intervals (PIs) around the prediction, which the range of actual y values we would expect for a given value of x.
  Challenge 4: if zombie weight is in pounds and zombie height is in inches, what is the expected height of a 150lbs zombie? What is the predicted difference in height between 180lbs and 220lbs zombies?
    > beta0<-t$Est[1]
    > beta1<-t$Est[2]
    > h_hat<-beta1*150+beta0
    > h_hat
    [1] 68.81825
    > h_hat_difference<-(beta1*220+beta0)-(beta1*180+beta0)
    > h_hat_difference
    [1] 7.800749
  The predict() function generates predicted, ie yhat, values for a vector of values of x. The second argument in the function includes the x variable name, and it's passed a vector of values.
    > m<-lm(data = d, height ~ weight)
    > h_hat<-predict(m, newdata = data.frame(weight = d$weight))
    > df<-data.frame(cbind(d$weight, d$height, h_hat))
    > names(df)<-c("x","y","yhat")
    > head(df)
             x        y     yhat
    1 132.0872 62.88951 65.32492
    2 146.3753 67.80277 68.11137
    3 152.9370 72.12908 69.39103
    4 129.7418 66.78484 64.86753
    5 132.4265 64.71832 65.39109
    6 152.5246 71.24326 69.31059
    > g<-ggplot(data = df, aes(x = x, y = yhat))
    > g<-g+geom_point()
    > g<-g+geom_point(aes(x = x, y = y), colour = "red")
    > g<-g+geom_segment(aes(x = x, y = yhat, xend = x, yend = y))
    > g: returns a graph [each x value has points at corresponding y and yhat values, connected by a vertical line]
  The vertical lines represent residuals, the difference between the observed and the fitted/predicted value of y at that given x.
  The predict() function also generates CIs around the predicted mean values for y values.
    > ci<-predict(m, newdata = data.frame(weight = 150), interval = "confidence", level = 0.95) [ETA: for a single value]
    > ci: returned three values, for fit/lwr/upr
    > ci<-predict(m, newdata = data.frame(weight = d$weight), interval = "confidence", level = 0.95) [ETA: for a vector of values]
    > head(ci): returns 18 values, 6 each for fit/lwr/upr
    > df<-cbind(df,ci)
    > names(df)<-c("x","y","yhat","CIfit","CIlwr","CIupr")
    > head(df): returns values for all six columns
    > g<-ggplot(data = df, aes(x = x, y = y))
    > g<-g+geom_point(alpha = 1/2)
    > g<-g+geom_line(aes(x = x, y = CIfit), colour = "black")
    > g<-g+geom_line(aes(x = x, y = CIlwr), colour = "blue")
    > g<-g+geom_line(aes(x = x, y = CIupr), colour = "blue")
    > g: returns a graph
  predict() can also generate prediction intervals (PIs) for values of y at each x.
    > pi<-predict(m, newdata = data.frame(weight = 150), interval = "prediction", level = 0.95)
    > pi: returns one value each for fit/lwr/upr
    > pi<-predict(m, newdata = data.frame(weight = d$weight), interval = "prediction", level = 0.95)
    > df<-cbind(df,pi)
    > names(df)<-c("x","y","yhat","CIfit","CIlwr","CIupr","PIfit","PIlwr","PIupr")
    > head(df)
    > g<-g+geom_line(data = df, aes(x = x, y = PIlwr), colour = "red")
    > g<-g+geom_line(data = df, aes(x = x, y = PIupr), colour = "red")
    > g: adds the PI lines to the graph
  Challenge 5: construct a linear model for the regression of zombie height on age and predict the mean height, the 95% CI around the predicted mean height, and the 95% PI around that mean for a vector of zombie ages (v<-seq(from=10, to=30, by=1)); then plot it.
    > v<-seq(from = 10, to = 30, by = 1)
    > m<-lm(data = d, height ~ age)
    > ci<-predict(m, newdata = data.frame(age = v), interval = "confidence", level = 0.95)
    > pi<-predict(m, newdata = data.frame(age = v), interval = "prediction", level = 0.95)
    > plot(data = d, height ~ age)
    > lines(x = v, y = ci[, 1], col = "black")
    > lines(x = v, y = ci[,2], col = "blue")
    > lines(x = v, y = ci[,3], col = "blue")
    > lines(x = v, y = pi[,2], col = "red")
    > lines(x = v, y = pi[,3], col = "red")
    > df<-data.frame(cbind(v,ci,pi))
    > names(df)<-c("age","CIfit","CIlwr","CIupr","PIfit","PIlwr","PIupr")
    > head(df)
    > g1<-ggplot(data = d, aes(x = age, y = height))
    > g1<-g1+geom_point(alpha = 1/2)
    > g1<-g1+geom_line(data = df, aes(x = v, y = CIfit), colour = "black", lwd = 1)
    > g1<-g1+geom_line(data = df, aes(x = v, y = CIlwr), colour = "blue")
    > g1<-g1+geom_line(data = df, aes(x = v, y = CIupr), colour = "blue")
    > g1<-g1+geom_line(data = df, aes(x = v, y = PIlwr), colour = "red")
    > g1<-g1+geom_line(data = df, aes(x = v, y = PIupr), colour = "red")
    > g2<-ggplot(data = d, aes(x = age, y = height))
    > g2<-g2+geom_point(alpha = 1/2)
    > g2<-g2+geom_smooth(method = "lm", formula = y ~ x)
    > grid.arrange(g1, g2, ncol = 2)
  Residuals: the model doesn't explain all of the variation in the dataset; the distance of each of these points from the predicted value for y for that value of x is the 'residual'.